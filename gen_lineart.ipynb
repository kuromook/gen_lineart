{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5a8a05-c7fc-40b7-9c24-247ed843ddeb",
   "metadata": {},
   "source": [
    "+ [x] Edge Loss ã®èª¿æ•´ï¼šw_edge ã‚’èª¿æ•´ã—ã¦â€œç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•â€ã¨â€œãƒã‚¤ã‚ºâ€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å–ã£ã¦ãã ã•ã„ï¼ˆ5ã€œ20ã®ç¯„å›²ã§è©¦ã™ã¨è‰¯ã„ï¼‰ã€‚\n",
    "\n",
    "+ [ ] Perceptual Lossï¼ˆVGGï¼‰ã‚’åŠ ãˆã‚‹ã¨ç·šã®å¤ªã•ã‚„â€œè¦‹ãŸç›®â€ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼ˆãŸã ã—å­¦ç¿’ãŒè¤‡é›‘ã«ï¼‰ã€‚torchvision.models.vgg19(pretrained=True) ã®ä¸­é–“ç‰¹å¾´ã§ L1 ã‚’å–ã‚‹æ–¹å¼ã€‚\n",
    "\n",
    "+ [ ] PatchGAN åˆ¤åˆ¥å™¨ï¼ˆGAN Lossï¼‰ ã‚’è¿½åŠ ã™ã‚‹ã¨æ›´ã«ã‚·ãƒ£ãƒ¼ãƒ—ã§è‡ªç„¶ãªç·šãŒå‡ºã›ã¾ã™ã€‚å®‰å®šåŒ–ã«ã¯ History Buffer/LSGAN/PatchGAN ãŒä½¿ã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Deep Supervisionï¼šè¤‡æ•°è§£åƒåº¦ã§å‡ºåŠ›ã—ã€ãã‚Œãã‚Œã«æå¤±ã‚’ä»˜ã‘ã‚‹ã¨ç´°ç·šã®ç¶­æŒã«æœ‰åŠ¹ã§ã™ã€‚\n",
    "\n",
    "+ [ ] Data augmentï¼šç·šã®å¤ªã•ã‚„æ˜åº¦å¤‰åŒ–ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ãªã©ã‚’å…¥ã‚Œã‚‹ã¨æ±åŒ–ãŒè‰¯ããªã‚Šã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Inference TTAï¼šå·¦å³åè»¢ã‚„ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¹³å‡åŒ–ã—ã¦æœ€çµ‚å‡ºåŠ›ã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df15c0cf-415f-4d0d-9a1d-b798d4af465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-11-15 14:09:38.373656713 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import math\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5344ef-5d27-4f53-b12d-d0afc940d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utility blocks\n",
    "# -----------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=not use_bn)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(ch, ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, 1, 1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(ch, affine=True)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm(out)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "# -----------------------\n",
    "# Simple Attention Gate (spatial attention)\n",
    "# -----------------------\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "    def forward(self, x):\n",
    "        # channel-wise average and max\n",
    "        avg = x.mean(dim=1, keepdim=True)\n",
    "        mx, _ = x.max(dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg, mx], dim=1)  # 2 x H x W\n",
    "        # reduce to single-channel attention\n",
    "        att = torch.sigmoid(self.conv(cat))\n",
    "        return x * att\n",
    "\n",
    "# -----------------------\n",
    "# Basic Self-Attention module (non-local)\n",
    "# -----------------------\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.key_conv   = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        proj_query = self.query_conv(x).view(b, -1, h*w).permute(0,2,1)  # B x N x C'\n",
    "        proj_key   = self.key_conv(x).view(b, -1, h*w)                    # B x C' x N\n",
    "        energy = torch.bmm(proj_query, proj_key)                          # B x N x N\n",
    "        att = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(b, -1, h*w)                  # B x C x N\n",
    "        out = torch.bmm(proj_value, att.permute(0,2,1))                   # B x C x N\n",
    "        out = out.view(b, c, h, w)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# U-Net with Residual and Attention\n",
    "# -----------------------\n",
    "class AttnResUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64, use_self_attn=True):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(ConvBlock(in_ch, base_ch), ResidualBlock(base_ch))\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(ConvBlock(base_ch, base_ch*2), ResidualBlock(base_ch*2))\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(ConvBlock(base_ch*2, base_ch*4), ResidualBlock(base_ch*4))\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(ConvBlock(base_ch*4, base_ch*8), ResidualBlock(base_ch*8))\n",
    "        self.self_attn = SelfAttention(base_ch*8) if use_self_attn else nn.Identity()\n",
    "\n",
    "        # decoder\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, 2)\n",
    "        self.dec3 = nn.Sequential(ConvBlock(base_ch*8, base_ch*4), ResidualBlock(base_ch*4))\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, 2)\n",
    "        self.dec2 = nn.Sequential(ConvBlock(base_ch*4, base_ch*2), ResidualBlock(base_ch*2))\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, 2)\n",
    "        self.dec1 = nn.Sequential(ConvBlock(base_ch*2, base_ch), ResidualBlock(base_ch))\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, out_ch, 1)\n",
    "        self.spatial_attn1 = SpatialAttention()\n",
    "        self.spatial_attn2 = SpatialAttention()\n",
    "        self.spatial_attn3 = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)           # B,64,H,W\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)          # B,128,H/2,W/2\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.enc3(p2)          # B,256,H/4,W/4\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        b = self.bottleneck(p3)     # B,512,H/8,W/8\n",
    "        b = self.self_attn(b)\n",
    "\n",
    "        u3 = self.up3(b)            # B,256,H/4,W/4\n",
    "        # attention on skip\n",
    "        e3_att = self.spatial_attn1(e3)\n",
    "        d3 = self.dec3(torch.cat([u3, e3_att], dim=1))\n",
    "\n",
    "        u2 = self.up2(d3)\n",
    "        e2_att = self.spatial_attn2(e2)\n",
    "        d2 = self.dec2(torch.cat([u2, e2_att], dim=1))\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        e1_att = self.spatial_attn3(e1)\n",
    "        d1 = self.dec1(torch.cat([u1, e1_att], dim=1))\n",
    "\n",
    "        out = self.final(d1)\n",
    "        out = torch.sigmoid(out)  # output in [0,1]\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Sobel (Edge) loss\n",
    "# -----------------------\n",
    "class SobelEdgeLoss(nn.Module):\n",
    "    def __init__(self, p=1):\n",
    "        super().__init__()\n",
    "        # Sobel kernels (horizontal, vertical)\n",
    "        kx = torch.tensor([[1, 0, -1],\n",
    "                           [2, 0, -2],\n",
    "                           [1, 0, -1]], dtype=torch.float32) / 4.0\n",
    "        ky = torch.tensor([[1, 2, 1],\n",
    "                           [0, 0, 0],\n",
    "                           [-1, -2, -1]], dtype=torch.float32) / 4.0\n",
    "        self.register_buffer('kx', kx.view(1,1,3,3))\n",
    "        self.register_buffer('ky', ky.view(1,1,3,3))\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred/target: Bx1xHxW\n",
    "        kx = self.kx.to(device=pred.device, dtype=pred.dtype)\n",
    "        ky = self.ky.to(device=pred.device, dtype=pred.dtype)\n",
    "\n",
    "        gx_pred = F.conv2d(pred, kx, padding=1)\n",
    "        gy_pred = F.conv2d(pred, ky, padding=1)\n",
    "        gx_tgt  = F.conv2d(target, kx, padding=1)\n",
    "        gy_tgt  = F.conv2d(target, ky, padding=1)\n",
    "\n",
    "        edge_pred = torch.sqrt(gx_pred**2 + gy_pred**2 + 1e-6)\n",
    "        edge_tgt  = torch.sqrt(gx_tgt**2 + gy_tgt**2 + 1e-6)\n",
    "\n",
    "        if self.p == 1:\n",
    "            return F.l1_loss(edge_pred, edge_tgt)\n",
    "        else:\n",
    "            return F.mse_loss(edge_pred, edge_tgt)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Small demo dataset (grayscale pairs)\n",
    "# Replace with your pair dataset: (rough_sketch, line_art)\n",
    "# -----------------------\n",
    "class PairImageDataset(Dataset):\n",
    "    def __init__(self, rough_dir, line_dir, size=(256,256)):\n",
    "        super().__init__()\n",
    "        self.rough_files = sorted(glob.glob(os.path.join(rough_dir, '*.jpg')))\n",
    "        self.line_files  = sorted(glob.glob(os.path.join(line_dir, '*.jpg')))\n",
    "        assert len(self.rough_files) == len(self.line_files), \"pair counts must match\"\n",
    "        self.tr = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    def __len__(self): return len(self.rough_files)\n",
    "    def __getitem__(self, idx):\n",
    "        r = Image.open(self.rough_files[idx]).convert('RGB')\n",
    "        l = Image.open(self.line_files[idx]).convert('RGB')\n",
    "        return self.tr(r), self.tr(l)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def train(\n",
    "    rough_dir='dataset/train/rough', line_dir='dataset/train/line',\n",
    "    epochs=50, batch_size=8, lr=1e-4, device='cuda'\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = PairImageDataset(rough_dir, line_dir)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=True).to(device)\n",
    "    edge_loss = SobelEdgeLoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(loader))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()  # mixed precision\n",
    "    save_dir = 'checkpoints'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # loss weights\n",
    "    w_l1 = 1.0\n",
    "    w_edge = 10.0   # emphasize edges\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (rough, line) in enumerate(loader):\n",
    "            rough = rough.to(device)\n",
    "            line = line.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(rough)\n",
    "                loss_l1 = l1_loss(out, line)\n",
    "                loss_edge = edge_loss(out, line)\n",
    "                loss = w_l1 * loss_l1 + w_edge * loss_edge\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 50 == 0:\n",
    "                print(f\"Epoch[{epoch}/{epochs}] Iter[{i+1}/{len(loader)}] Loss: {running_loss / (i+1):.4f}\")\n",
    "\n",
    "        avg = running_loss / len(loader)\n",
    "        print(f\"==> Epoch {epoch} finished. avg loss: {avg:.4f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict()\n",
    "        }, os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce6a6d5e-7092-45d6-8705-ead8e0319174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 finished. avg loss: 0.9017\n",
      "==> Epoch 2 finished. avg loss: 0.7092\n",
      "==> Epoch 3 finished. avg loss: 0.6463\n",
      "==> Epoch 4 finished. avg loss: 0.6163\n",
      "==> Epoch 5 finished. avg loss: 0.5990\n",
      "==> Epoch 6 finished. avg loss: 0.5880\n",
      "==> Epoch 7 finished. avg loss: 0.5804\n",
      "==> Epoch 8 finished. avg loss: 0.5737\n",
      "==> Epoch 9 finished. avg loss: 0.5687\n",
      "==> Epoch 10 finished. avg loss: 0.5645\n",
      "==> Epoch 11 finished. avg loss: 0.5611\n",
      "==> Epoch 12 finished. avg loss: 0.5586\n",
      "==> Epoch 13 finished. avg loss: 0.5554\n",
      "==> Epoch 14 finished. avg loss: 0.5540\n",
      "==> Epoch 15 finished. avg loss: 0.5514\n",
      "==> Epoch 16 finished. avg loss: 0.5491\n",
      "==> Epoch 17 finished. avg loss: 0.5450\n",
      "==> Epoch 18 finished. avg loss: 0.5417\n",
      "==> Epoch 19 finished. avg loss: 0.5370\n",
      "==> Epoch 20 finished. avg loss: 0.5299\n",
      "==> Epoch 21 finished. avg loss: 0.5224\n",
      "==> Epoch 22 finished. avg loss: 0.5123\n",
      "==> Epoch 23 finished. avg loss: 0.5016\n",
      "==> Epoch 24 finished. avg loss: 0.4891\n",
      "==> Epoch 25 finished. avg loss: 0.4769\n",
      "==> Epoch 26 finished. avg loss: 0.4645\n",
      "==> Epoch 27 finished. avg loss: 0.4540\n",
      "==> Epoch 28 finished. avg loss: 0.4458\n",
      "==> Epoch 29 finished. avg loss: 0.4413\n",
      "==> Epoch 30 finished. avg loss: 0.4396\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: point to your directories containing paired PNGs\n",
    "    train(\n",
    "        rough_dir='dataset/train/rough',   # ä¸‹çµµç”»åƒãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpngï¼‰\n",
    "        line_dir='dataset/train/line',     # æ­£è§£ç·šç”»ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpngï¼‰\n",
    "        epochs=30,\n",
    "        batch_size=8,\n",
    "        lr=2e-4,\n",
    "        device='cuda'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d36615-7b3a-4232-af7e-410c5dd516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "def load_model(checkpoint_path, device='cuda'):\n",
    "    model = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=True)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"âœ… Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img_path, size=(256,256)):\n",
    "    tr = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return tr(img).unsqueeze(0)  # 1x1xHxW tensor\n",
    "\n",
    "def postprocess_and_save(tensor, save_path):\n",
    "    # tensor: 1x1xHxW in [0,1]\n",
    "    img = tensor.squeeze(0).squeeze(0).detach().cpu()  # HxW\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.save(save_path)\n",
    "    print(f\"ğŸ’¾ Saved output to {save_path}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(model, input_path, output_path, device='cuda'):\n",
    "    img = preprocess_image(input_path)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        output = model(img)\n",
    "\n",
    "    postprocess_and_save(output, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537d6a51-9264-41a3-a731-4f66037a3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_30.pth\n",
      "ğŸ’¾ Saved output to results/sample.jpg\n",
      "ğŸ’¾ Saved output to results/sample01.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ======== è¨­å®š =========\n",
    "    checkpoint_path = \"checkpoints/model_epoch_30.pth\"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n",
    "    input_dir = \"test/rough\"   # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸‹çµµ\n",
    "    output_dir = \"results\"             # å‡ºåŠ›å…ˆ\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = load_model(checkpoint_path, device)\n",
    "\n",
    "    # ======== æ¨è«– =========\n",
    "    rough_files = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    for fname in rough_files:\n",
    "        in_path = os.path.join(input_dir, fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        inference(model, in_path, out_path, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf0af1-ea5e-456d-8c08-7dd674e87b92",
   "metadata": {},
   "source": [
    "| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿    | åŠ¹æœ                     | å‚™è€ƒ                     |\n",
    "| -------- | ---------------------- | ---------------------- |\n",
    "| `w_l1`   | å…¨ä½“ã®æ˜ã‚‹ã•ã‚„å½¢ã®å†ç¾é‡è¦–          | é€šå¸¸ 1.0 ã§OK             |\n",
    "| `w_edge` | ã‚¨ãƒƒã‚¸ï¼ˆç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•ï¼‰ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ | å¤§ãã„ã»ã©ç·šãŒãã£ãã‚Šã€‚ãŸã ã—ãƒã‚¤ã‚ºã‚‚å¢—ãˆã‚‹ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a2260-cc1e-463b-b91e-c9205c962946",
   "metadata": {},
   "source": [
    "| ä¾‹      | è¨­å®š               | åŠ¹æœã®å‚¾å‘                 |\n",
    "| ------ | ---------------- | --------------------- |\n",
    "| æŸ”ã‚‰ã‹ã‚   | `w_edge = 3.0`   | ç·šãŒã‚„ã‚„ã¼ã‚„ã‘ã‚‹ãŒãƒã‚¤ã‚ºå°‘ãªã„       |\n",
    "| æ¨™æº–     | `w_edge = 5.0`   | ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆä»Šã®è¨­å®šï¼‰           |\n",
    "| ã‚·ãƒ£ãƒ¼ãƒ—å¼·èª¿ | `w_edge = 10.0`  | ç·šãŒãã£ãã‚Šã€ã‚„ã‚„ãƒã‚¤ã‚ºå‡ºã‚‹        |\n",
    "| å¼·ã‚     | `w_edge = 15ã€œ20` | ã‚¨ãƒƒã‚¸æœ€é‡è¦–ã€ç·šã¯æ¿ƒã„ãŒãƒã‚¤ã‚ºãŒå¢—ãˆã‚‹å‚¾å‘ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68474863-1bb7-4e69-9174-c2e6882f2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "drwxrwxr-x  2 sh1 sh1  4096 11æœˆ 15 14:04 .\n",
      "drwxrwxr-x 13 sh1 sh1  4096 11æœˆ 15 14:04 ..\n",
      "-rw-rw-r--  1 sh1 sh1 11303 11æœˆ 13 19:07 result_line.png\n",
      "-rw-rw-r--  1 sh1 sh1  3360 11æœˆ 15 14:04 sample01.jpg\n",
      "-rw-rw-r--  1 sh1 sh1  7018 11æœˆ 14 02:18 sample_w_edge10.jpg\n",
      "-rw-rw-r--  1 sh1 sh1  5132 11æœˆ 14 02:01 sample_w_edge20.jpg\n",
      "-rw-rw-r--  1 sh1 sh1  5574 11æœˆ 15 14:04 sample_w_edge8.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls -la results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890dab9-fb7f-497d-a3a0-4ff0551fac16",
   "metadata": {},
   "source": [
    "result\n",
    "![result](./results/sample_w_edge8.jpg) 8\n",
    "![result](./results/sample_w_edge10.jpg) 10\n",
    "![result_w_edge20](./results/sample_w_edge20.jpg) 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2721061-310d-4cb1-ab55-c83b623e9196",
   "metadata": {},
   "source": [
    "sample  \n",
    "![sample](./test/rough/sample.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11e60ee-d3be-4e93-914f-ab2efec1d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "def export_to_onnx(model, input_size, onnx_path=\"model.onnx\"):\n",
    "    # 1. CPUã¸ç§»å‹•\n",
    "    model = model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # 2. å…¥åŠ›ã‚‚CPU\n",
    "    x = torch.randn(*input_size, device=\"cpu\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model, x, onnx_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output']\n",
    "    )\n",
    "    print(f\"[OK] Exported ONNX -> {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddddc29-ab5b-4925-af0e-0c3057123fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_20.pth\n",
      "[OK] Exported ONNX -> model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh1/deepl/mynovel/venv/lib/python3.10/site-packages/torch/onnx/symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"checkpoints/model_epoch_20.pth\")\n",
    "export_to_onnx(model, (1,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cea07c-1e27-40d7-bc21-6a6d7db15076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttention(512)\n",
    "dummy = torch.randn(1, 512, 32, 32)\n",
    "\n",
    "traced = torch.jit.trace(model, dummy)\n",
    "traced.save(\"selfattention.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f1561-299d-4029-bb1e-6c3c5d7180cc",
   "metadata": {},
   "source": [
    "![attn](./graph/selfattention.pt.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a658467-fbde-4f1e-9923-da9046738e2b",
   "metadata": {},
   "source": [
    "ğŸ” ã“ã® Self-Attention ã¯ä½•ï¼Ÿ\n",
    "\n",
    "ã“ã‚Œã¯ Non-Local Attentionï¼ˆSelf-Attentionï¼‰ ã®å…¸å‹å®Ÿè£…ã§ã€\n",
    "ç‰¹å¾´ãƒãƒƒãƒ—å†…ã® ã™ã¹ã¦ã®ä½ç½®åŒå£«ã®é–¢é€£æ€§ã‚’è¨ˆç®—ã—ã¦ã€\n",
    "ã€Œã©ã“ãŒã©ã“ã¨é–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã€ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "ç·šç”»ç”Ÿæˆã ã¨ï¼š\n",
    "\n",
    "é ãé›¢ã‚ŒãŸç·šã®æ–¹å‘æ€§\n",
    "\n",
    "åŒã˜æ§‹é€ ã®åå¾©\n",
    "\n",
    "é•·ã„è¼ªéƒ­ç·šã®ã¤ãªãŒã‚Š\n",
    "\n",
    "ãªã©ã‚’è£œå®Œã§ãã‚‹ã®ã§ U-Net ã«ã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f1790-3e37-4600-9d62-1495327b75b5",
   "metadata": {},
   "source": [
    "Self-Attentionï¼ˆNon-Local Blockï¼‰ã‚’ U-Net ã«è¿½åŠ ã—ãŸå ´åˆã€å®Ÿè³ªçš„ã« â€œæ–°ã—ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã»ã¼ç”Ÿã¾ã‚Œã¾ã›ã‚“ã€‚â€\n",
    "ãŸã ã—ã€è¨­ç½®ä½ç½®ãƒ»æ•°ãƒ»in_dimï¼ˆãƒãƒ£ãƒãƒ«ï¼‰ãƒ»å­¦ç¿’ç‡ã¸ã®å½±éŸ¿ ã¨ã„ã† â€œæº–ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿â€ ã¯å­˜åœ¨ã—ã¾ã™ã€‚ \n",
    "\n",
    "â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆæº–ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
    "\n",
    "ã©ã®å±¤ã«å…¥ã‚Œã‚‹ã‹ï¼ˆæœ€é‡è¦ï¼‰\n",
    "\n",
    "ä½•å€‹å…¥ã‚Œã‚‹ã‹\n",
    "\n",
    "å­¦ç¿’ç‡ã‚’å¤‰ãˆã‚‹ã‹ï¼ˆå ´åˆã«ã‚ˆã‚Šï¼‰\n",
    "\n",
    "in_dimï¼ˆãƒãƒ£ãƒãƒ«æ•°ï¼‰ã®åˆã†å±¤ã‚’é¸ã¶å¿…è¦\n",
    "\n",
    "â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´ã—ãªãã¦ã‚ˆã„ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãªã„ï¼‰\n",
    "\n",
    "Î³\n",
    "\n",
    "attention ã®æ¸©åº¦\n",
    "\n",
    "head æ•°\n",
    "\n",
    "dropout\n",
    "\n",
    "kernel size\n",
    "\n",
    "scaling factor\n",
    "\n",
    "ã“ã‚Œã‚‰ã¯ã™ã¹ã¦è‡ªå‹•ã§æ©Ÿèƒ½ã™ã‚‹ã®ã§è§¦ã‚‰ãªã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1f2dbb-bbfd-4920-9385-01efae2610a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attnres_graph.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttnResUNet()\n",
    "dummy = torch.randn(1,1,256,256)\n",
    "\n",
    "out = model(dummy)\n",
    "dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "dot.render(\"attnres_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9651b30-942d-4c75-a84d-62557b36039d",
   "metadata": {},
   "source": [
    "![attnres](./graph/attnres_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d46e5d-059c-4ee0-837c-e6870a99018f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
