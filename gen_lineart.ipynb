{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5a8a05-c7fc-40b7-9c24-247ed843ddeb",
   "metadata": {},
   "source": [
    "+ [x] Edge Loss ã®èª¿æ•´ï¼šw_edge ã‚’èª¿æ•´ã—ã¦â€œç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•â€ã¨â€œãƒã‚¤ã‚ºâ€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å–ã£ã¦ãã ã•ã„ï¼ˆ5ã€œ20ã®ç¯„å›²ã§è©¦ã™ã¨è‰¯ã„ï¼‰ã€‚\n",
    "\n",
    "+ [ ] Perceptual Lossï¼ˆVGGï¼‰ã‚’åŠ ãˆã‚‹ã¨ç·šã®å¤ªã•ã‚„â€œè¦‹ãŸç›®â€ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼ˆãŸã ã—å­¦ç¿’ãŒè¤‡é›‘ã«ï¼‰ã€‚torchvision.models.vgg19(pretrained=True) ã®ä¸­é–“ç‰¹å¾´ã§ L1 ã‚’å–ã‚‹æ–¹å¼ã€‚\n",
    "\n",
    "+ [ ] PatchGAN åˆ¤åˆ¥å™¨ï¼ˆGAN Lossï¼‰ ã‚’è¿½åŠ ã™ã‚‹ã¨æ›´ã«ã‚·ãƒ£ãƒ¼ãƒ—ã§è‡ªç„¶ãªç·šãŒå‡ºã›ã¾ã™ã€‚å®‰å®šåŒ–ã«ã¯ History Buffer/LSGAN/PatchGAN ãŒä½¿ã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Deep Supervisionï¼šè¤‡æ•°è§£åƒåº¦ã§å‡ºåŠ›ã—ã€ãã‚Œãã‚Œã«æå¤±ã‚’ä»˜ã‘ã‚‹ã¨ç´°ç·šã®ç¶­æŒã«æœ‰åŠ¹ã§ã™ã€‚\n",
    "\n",
    "+ [ ] Data augmentï¼šç·šã®å¤ªã•ã‚„æ˜åº¦å¤‰åŒ–ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ãªã©ã‚’å…¥ã‚Œã‚‹ã¨æ±åŒ–ãŒè‰¯ããªã‚Šã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Inference TTAï¼šå·¦å³åè»¢ã‚„ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¹³å‡åŒ–ã—ã¦æœ€çµ‚å‡ºåŠ›ã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "df15c0cf-415f-4d0d-9a1d-b798d4af465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import math\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb5344ef-5d27-4f53-b12d-d0afc940d3f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utility blocks\n",
    "# -----------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=not use_bn)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(ch, ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, 1, 1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(ch, affine=True)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm(out)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "# -----------------------\n",
    "# Simple Attention Gate (spatial attention)\n",
    "# -----------------------\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "    def forward(self, x):\n",
    "        # channel-wise average and max\n",
    "        avg = x.mean(dim=1, keepdim=True)\n",
    "        mx, _ = x.max(dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg, mx], dim=1)  # 2 x H x W\n",
    "        # reduce to single-channel attention\n",
    "        att = torch.sigmoid(self.conv(cat))\n",
    "        return x * att\n",
    "\n",
    "# -----------------------\n",
    "# Basic Self-Attention module (non-local)\n",
    "# -----------------------\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.key_conv   = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        proj_query = self.query_conv(x).view(b, -1, h*w).permute(0,2,1)  # B x N x C'\n",
    "        proj_key   = self.key_conv(x).view(b, -1, h*w)                    # B x C' x N\n",
    "        energy = torch.bmm(proj_query, proj_key)                          # B x N x N\n",
    "        att = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(b, -1, h*w)                  # B x C x N\n",
    "        out = torch.bmm(proj_value, att.permute(0,2,1))                   # B x C x N\n",
    "        out = out.view(b, c, h, w)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Sobel (Edge) loss\n",
    "# -----------------------\n",
    "class SobelEdgeLoss(nn.Module):\n",
    "    def __init__(self, p=1):\n",
    "        super().__init__()\n",
    "        # Sobel kernels (horizontal, vertical)\n",
    "        kx = torch.tensor([[1, 0, -1],\n",
    "                           [2, 0, -2],\n",
    "                           [1, 0, -1]], dtype=torch.float32) / 4.0\n",
    "        ky = torch.tensor([[1, 2, 1],\n",
    "                           [0, 0, 0],\n",
    "                           [-1, -2, -1]], dtype=torch.float32) / 4.0\n",
    "        self.register_buffer('kx', kx.view(1,1,3,3))\n",
    "        self.register_buffer('ky', ky.view(1,1,3,3))\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred/target: Bx1xHxW\n",
    "        kx = self.kx.to(device=pred.device, dtype=pred.dtype)\n",
    "        ky = self.ky.to(device=pred.device, dtype=pred.dtype)\n",
    "\n",
    "        gx_pred = F.conv2d(pred, kx, padding=1)\n",
    "        gy_pred = F.conv2d(pred, ky, padding=1)\n",
    "        gx_tgt  = F.conv2d(target, kx, padding=1)\n",
    "        gy_tgt  = F.conv2d(target, ky, padding=1)\n",
    "\n",
    "        edge_pred = torch.sqrt(gx_pred**2 + gy_pred**2 + 1e-6)\n",
    "        edge_tgt  = torch.sqrt(gx_tgt**2 + gy_tgt**2 + 1e-6)\n",
    "\n",
    "        if self.p == 1:\n",
    "            return F.l1_loss(edge_pred, edge_tgt)\n",
    "        else:\n",
    "            return F.mse_loss(edge_pred, edge_tgt)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Small demo dataset (grayscale pairs)\n",
    "# Replace with your pair dataset: (rough_sketch, line_art)\n",
    "# -----------------------\n",
    "class PairImageDataset(Dataset):\n",
    "    def __init__(self, rough_dir, line_dir, size=(256,256)):\n",
    "        super().__init__()\n",
    "        self.rough_files = sorted(glob.glob(os.path.join(rough_dir, '*.jpg')))\n",
    "        self.line_files  = sorted(glob.glob(os.path.join(line_dir, '*.jpg')))\n",
    "        assert len(self.rough_files) == len(self.line_files), \"pair counts must match\"\n",
    "        self.tr = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    def __len__(self): return len(self.rough_files)\n",
    "    def __getitem__(self, idx):\n",
    "        r = Image.open(self.rough_files[idx]).convert('RGB')\n",
    "        l = Image.open(self.line_files[idx]).convert('RGB')\n",
    "\n",
    "        r = self.tr(r)\n",
    "        l = self.tr(l)\n",
    "\n",
    "        # --- ã“ã“ã§åè»¢ ---\n",
    "        #l = 1.0 - l   # â† ç™½ç·šâ†’é»’ç·šã«åè»¢\n",
    "\n",
    "        return r, l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92892ad2-2106-4bd0-b25c-e888f12f969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# U-Net with Residual and Attention\n",
    "# -----------------------\n",
    "class AttnResUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64, use_self_attn=True):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(ConvBlock(in_ch, base_ch), ResidualBlock(base_ch))\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(ConvBlock(base_ch, base_ch*2), ResidualBlock(base_ch*2))\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(ConvBlock(base_ch*2, base_ch*4), ResidualBlock(base_ch*4))\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(ConvBlock(base_ch*4, base_ch*8), ResidualBlock(base_ch*8))\n",
    "        self.self_attn = SelfAttention(base_ch*8) if use_self_attn else nn.Identity()\n",
    "\n",
    "        # decoder\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, 2)\n",
    "        self.dec3 = nn.Sequential(ConvBlock(base_ch*8, base_ch*4), ResidualBlock(base_ch*4))\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, 2)\n",
    "        self.dec2 = nn.Sequential(ConvBlock(base_ch*4, base_ch*2), ResidualBlock(base_ch*2))\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, 2)\n",
    "        self.dec1 = nn.Sequential(ConvBlock(base_ch*2, base_ch), ResidualBlock(base_ch))\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, out_ch, 1)\n",
    "\n",
    "        self.spatial_attn1 = SpatialAttention()\n",
    "        self.spatial_attn2 = SpatialAttention()\n",
    "        self.spatial_attn3 = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)           # B,64,H,W\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)          # B,128,H/2,W/2\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.enc3(p2)          # B,256,H/4,W/4\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        b = self.bottleneck(p3)     # B,512,H/8,W/8\n",
    "        b = self.self_attn(b)\n",
    "\n",
    "        u3 = self.up3(b)            # B,256,H/4,W/4\n",
    "        # attention on skip\n",
    "        #e3_att = self.spatial_attn1(e3)\n",
    "        e3_att = e3\n",
    "        d3 = self.dec3(torch.cat([u3, e3_att], dim=1))\n",
    "\n",
    "        u2 = self.up2(d3)\n",
    "        #e2_att = self.spatial_attn2(e2)\n",
    "        e2_att = e2\n",
    "        d2 = self.dec2(torch.cat([u2, e2_att], dim=1))\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        #e1_att = self.spatial_attn3(e1)\n",
    "        e1_att = e1\n",
    "        d1 = self.dec1(torch.cat([u1, e1_att], dim=1))\n",
    "\n",
    "        out = torch.sigmoid(self.final(d1))\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae6cfd18-877a-4c32-b183-889029b28f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "# PatchGAN Discriminator (70x70-ish style)\n",
    "# -----------------------\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=2, base_ch=64, n_layers=4):\n",
    "        \"\"\"\n",
    "        in_ch: å…¥åŠ›ãƒãƒ£ãƒãƒ«ï¼ˆé€šå¸¸ã¯ concat(rough, line) -> 2ch, ã‚‚ã—ãã¯ 1ch + 1ch = 2ï¼‰\n",
    "        base_ch: æœ€åˆã®ãƒãƒ£ãƒãƒ«æ•°\n",
    "        n_layers: ç•³ã¿è¾¼ã¿æ®µæ•°ï¼ˆç©ºé–“ç¸®å°å›æ•°ï¼‰\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # Conv 1: no norm, leakyrelu\n",
    "        layers.append(nn.Conv2d(in_ch, base_ch, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        ch = base_ch\n",
    "        # middle layers\n",
    "        for i in range(1, n_layers):\n",
    "            in_c = ch\n",
    "            out_c = min(ch * 2, 512)\n",
    "            stride = 2 if i < n_layers-1 else 1  # æœ€å¾Œã®ç•³ã¿è¾¼ã¿ã¯ stride=1 ã«ã—ã¦ patch å‡ºåŠ›ã®è§£åƒåº¦ã‚’ä¿ã¤\n",
    "            layers.append(nn.Conv2d(in_c, out_c, kernel_size=4, stride=stride, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(out_c, affine=True))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            ch = out_c\n",
    "\n",
    "        # final conv -> 1 channel output (patch map)\n",
    "        layers.append(nn.Conv2d(ch, 1, kernel_size=4, stride=1, padding=1))\n",
    "        # NOTE: no sigmoid here for LSGAN (we'll use MSELoss directly)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # shape B x 1 x H_patch x W_patch\n",
    "\n",
    "# -----------------------\n",
    "# Replay (History) Buffer\n",
    "# -----------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, items):\n",
    "        \"\"\"\n",
    "        items: tensor (B, C, H, W)\n",
    "        returns: tensor (B, C, H, W) where some items may be replaced by older ones\n",
    "        \"\"\"\n",
    "        returned = []\n",
    "        for item in items:\n",
    "            item = torch.unsqueeze(item.data, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                # fill buffer\n",
    "                self.data.append(item)\n",
    "                returned.append(item)\n",
    "            else:\n",
    "                if random.random() > 0.5:\n",
    "                    # use a previously stored one\n",
    "                    idx = random.randint(0, self.max_size - 1)\n",
    "                    tmp = self.data[idx].clone()\n",
    "                    self.data[idx] = item\n",
    "                    returned.append(tmp)\n",
    "                else:\n",
    "                    # use current\n",
    "                    returned.append(item)\n",
    "        return torch.cat(returned, dim=0)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Training loop with PatchGAN + LSGAN\n",
    "# -----------------------\n",
    "def train_with_gan(\n",
    "    rough_dir='dataset/train/rough',\n",
    "    line_dir='dataset/train/line',\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    lr_g=1e-4,\n",
    "    lr_d=4e-4,\n",
    "    device='cuda',\n",
    "    use_history_buffer=True\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = PairImageDataset(rough_dir, line_dir)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # models\n",
    "    G = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=False).to(device)\n",
    "    D = PatchDiscriminator(in_ch=2, base_ch=64, n_layers=3).to(device)\n",
    "\n",
    "    # losses\n",
    "    mse_loss = nn.MSELoss()      # LSGAN\n",
    "    l1_loss = nn.L1Loss()\n",
    "    edge_loss = SobelEdgeLoss()\n",
    "\n",
    "    # optimizers\n",
    "    opt_G = torch.optim.AdamW(G.parameters(), lr=lr_g, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "    opt_D = torch.optim.AdamW(D.parameters(), lr=lr_d, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "\n",
    "    # schedulers (optional)\n",
    "    scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(opt_G, T_max=epochs*len(loader))\n",
    "    scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(opt_D, T_max=epochs*len(loader))\n",
    "\n",
    "    # mixed precision\n",
    "    scaler_G = torch.cuda.amp.GradScaler()\n",
    "    scaler_D = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # history buffer for fake images\n",
    "    replay_buffer = ReplayBuffer(max_size=50) if use_history_buffer else None\n",
    "\n",
    "    save_dir = 'checkpoints_gan'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # weights\n",
    "    w_l1 = 5\n",
    "    w_edge = 10\n",
    "    w_gan = 5  # weight for adversarial loss (generator)\n",
    "\n",
    "    # labels for LSGAN\n",
    "    real_label_value = 1.0\n",
    "    fake_label_value = 0.0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        G.train()\n",
    "        D.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (rough, line) in enumerate(loader):\n",
    "            rough = rough.to(device, non_blocking=True)\n",
    "            line = line.to(device, non_blocking=True)\n",
    "\n",
    "            # --------------------\n",
    "            # 1) Update D: maximize L_lsgan(D) = 0.5 * E[(D(x_real)-1)^2] + 0.5 * E[(D(x_fake)-0)^2]\n",
    "            # --------------------\n",
    "            opt_D.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # real pair: concat(rough, line)\n",
    "                real_input = torch.cat([rough, line], dim=1)  # B x 2 x H x W\n",
    "                pred_real = D(real_input)\n",
    "                # target for real: ones with same shape as pred_real\n",
    "                valid = torch.full_like(pred_real, real_label_value, device=device)\n",
    "\n",
    "                loss_D_real = mse_loss(pred_real, valid)\n",
    "\n",
    "                # fake pair: concat(rough, G(rough))\n",
    "                fake = G(rough).detach()  # detach so gradients aren't backprop to G\n",
    "                fake_input = torch.cat([rough, fake], dim=1)\n",
    "\n",
    "                # optionally use history buffer\n",
    "                if replay_buffer is not None:\n",
    "                    fake_to_D = replay_buffer.push_and_pop([f for f in fake])\n",
    "                    # push_and_pop returns cpu/gpu tensors depending; ensure it's on device\n",
    "                    fake_to_D = fake_to_D.to(device)\n",
    "                    fake_input = torch.cat([rough, fake_to_D], dim=1)\n",
    "                pred_fake = D(fake_input)\n",
    "                fake_label = torch.full_like(pred_fake, fake_label_value, device=device)\n",
    "                loss_D_fake = mse_loss(pred_fake, fake_label)\n",
    "\n",
    "                loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "\n",
    "            scaler_D.scale(loss_D).backward()\n",
    "            scaler_D.step(opt_D)\n",
    "            scaler_D.update()\n",
    "            scheduler_D.step()\n",
    "\n",
    "            # --------------------\n",
    "            # 2) Update G: minimize adversarial + L1 + Edge\n",
    "            # --------------------\n",
    "            opt_G.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                gen = G(rough)\n",
    "                # adversarial loss (want D(concat(rough, gen)) -> 1)\n",
    "                pred_gen = D(torch.cat([rough, gen], dim=1))\n",
    "                valid_for_gen = torch.full_like(pred_gen, real_label_value, device=device)\n",
    "                loss_gan = mse_loss(pred_gen, valid_for_gen)\n",
    "\n",
    "                # other losses\n",
    "                loss_l1 = l1_loss(gen, line)\n",
    "                loss_edge = edge_loss(gen, line)\n",
    "\n",
    "                loss_G = w_gan * loss_gan + w_l1 * loss_l1 + w_edge * loss_edge\n",
    "\n",
    "            scaler_G.scale(loss_G).backward()\n",
    "            scaler_G.step(opt_G)\n",
    "            scaler_G.update()\n",
    "            scheduler_G.step()\n",
    "\n",
    "            running_loss += loss_G.item()\n",
    "\n",
    "            if (i+1) % 50 == 0:\n",
    "                avg = running_loss / (i+1)\n",
    "                print(f\"Epoch[{epoch}/{epochs}] Iter[{i+1}/{len(loader)}] G_loss: {avg:.4f} (gan:{loss_gan.item():.4f} l1:{loss_l1.item():.4f} edge:{loss_edge.item():.4f})\")\n",
    "\n",
    "        avg_epoch = running_loss / len(loader)\n",
    "        print(f\"==> Epoch {epoch} finished. avg G loss: {avg_epoch:.4f}\")\n",
    "\n",
    "        # save checkpoint (both G and D)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_state': G.state_dict(),\n",
    "            'D_state': D.state_dict(),\n",
    "            'optG': opt_G.state_dict(),\n",
    "            'optD': opt_D.state_dict()\n",
    "        }, os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
    "\n",
    "    print(\"GAN Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce6a6d5e-7092-45d6-8705-ead8e0319174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 1 finished. avg G loss: 6.4404\n",
      "==> Epoch 2 finished. avg G loss: 5.1592\n",
      "==> Epoch 3 finished. avg G loss: 4.7803\n",
      "==> Epoch 4 finished. avg G loss: 4.5562\n",
      "==> Epoch 5 finished. avg G loss: 4.4045\n",
      "==> Epoch 6 finished. avg G loss: 4.3164\n",
      "==> Epoch 7 finished. avg G loss: 4.2360\n",
      "==> Epoch 8 finished. avg G loss: 4.1839\n",
      "==> Epoch 9 finished. avg G loss: 4.1428\n",
      "==> Epoch 10 finished. avg G loss: 4.1041\n",
      "==> Epoch 11 finished. avg G loss: 4.0694\n",
      "==> Epoch 12 finished. avg G loss: 4.0427\n",
      "==> Epoch 13 finished. avg G loss: 4.0225\n",
      "==> Epoch 14 finished. avg G loss: 4.0078\n",
      "==> Epoch 15 finished. avg G loss: 3.9966\n",
      "==> Epoch 16 finished. avg G loss: 3.9892\n",
      "==> Epoch 17 finished. avg G loss: 3.9850\n",
      "==> Epoch 18 finished. avg G loss: 3.9833\n",
      "GAN Training finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: point to your directories containing paired PNGs\n",
    "    train_with_gan(\n",
    "        rough_dir='dataset/train/rough',\n",
    "        line_dir='dataset/train/line',\n",
    "        epochs=18, # æ™‚çŸ­ã®ãŸã‚30->18ã«å¤‰æ›´\n",
    "        batch_size=8,\n",
    "        lr_g=2e-4,   # â† Generator ã® lr\n",
    "        lr_d=0e-4,   # â† Discriminator ã® lr\n",
    "        device='cuda'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "71d36615-7b3a-4232-af7e-410c5dd516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "def load_model(checkpoint_path, device='cuda'):\n",
    "    model = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=True)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"âœ… Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img_path, size=(256,256)):\n",
    "    tr = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return tr(img).unsqueeze(0)  # 1x1xHxW tensor\n",
    "\n",
    "def postprocess_and_save(tensor, save_path):\n",
    "    # tensor: 1x1xHxW in [0,1]\n",
    "    img = tensor.squeeze(0).squeeze(0).detach().cpu()  # HxW\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.save(save_path)\n",
    "    print(f\"ğŸ’¾ Saved output to {save_path}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(model, input_path, output_path, device='cuda'):\n",
    "    img = preprocess_image(input_path)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        output = model(img)\n",
    "\n",
    "    postprocess_and_save(output, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "537d6a51-9264-41a3-a731-4f66037a3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_18.pth\n",
      "ğŸ’¾ Saved output to results/sample.jpg\n",
      "ğŸ’¾ Saved output to results/sample01.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ======== è¨­å®š =========\n",
    "    checkpoint_path = \"checkpoints/model_epoch_18.pth\"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« # æ™‚çŸ­ã®ãŸã‚30->18ã«å¤‰æ›´\n",
    "    input_dir = \"test/rough\"   # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸‹çµµ\n",
    "    output_dir = \"results\"             # å‡ºåŠ›å…ˆ\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = load_model(checkpoint_path, device)\n",
    "\n",
    "    # ======== æ¨è«– =========\n",
    "    rough_files = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    for fname in rough_files:\n",
    "        in_path = os.path.join(input_dir, fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        inference(model, in_path, out_path, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf0af1-ea5e-456d-8c08-7dd674e87b92",
   "metadata": {},
   "source": [
    "| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿    | åŠ¹æœ                     | å‚™è€ƒ                     |\n",
    "| -------- | ---------------------- | ---------------------- |\n",
    "| `w_l1`   | å…¨ä½“ã®æ˜ã‚‹ã•ã‚„å½¢ã®å†ç¾é‡è¦–          | é€šå¸¸ 1.0 ã§OK             |\n",
    "| `w_edge` | ã‚¨ãƒƒã‚¸ï¼ˆç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•ï¼‰ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ | å¤§ãã„ã»ã©ç·šãŒãã£ãã‚Šã€‚ãŸã ã—ãƒã‚¤ã‚ºã‚‚å¢—ãˆã‚‹ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a2260-cc1e-463b-b91e-c9205c962946",
   "metadata": {},
   "source": [
    "| ä¾‹      | è¨­å®š               | åŠ¹æœã®å‚¾å‘                 |\n",
    "| ------ | ---------------- | --------------------- |\n",
    "| æŸ”ã‚‰ã‹ã‚   | `w_edge = 3.0`   | ç·šãŒã‚„ã‚„ã¼ã‚„ã‘ã‚‹ãŒãƒã‚¤ã‚ºå°‘ãªã„       |\n",
    "| æ¨™æº–     | `w_edge = 5.0`   | ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆä»Šã®è¨­å®šï¼‰           |\n",
    "| ã‚·ãƒ£ãƒ¼ãƒ—å¼·èª¿ | `w_edge = 10.0`  | ç·šãŒãã£ãã‚Šã€ã‚„ã‚„ãƒã‚¤ã‚ºå‡ºã‚‹        |\n",
    "| å¼·ã‚     | `w_edge = 15ã€œ20` | ã‚¨ãƒƒã‚¸æœ€é‡è¦–ã€ç·šã¯æ¿ƒã„ãŒãƒã‚¤ã‚ºãŒå¢—ãˆã‚‹å‚¾å‘ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890dab9-fb7f-497d-a3a0-4ff0551fac16",
   "metadata": {},
   "source": [
    "result\n",
    "![result](./results/sample_w_edge6.jpg) 6\n",
    "![result](./results/sample_w_edge8.jpg) 8\n",
    "![result](./results/sample_w_edge10.jpg) 10\n",
    "![result_w_edge20](./results/sample_w_edge20.jpg) 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2721061-310d-4cb1-ab55-c83b623e9196",
   "metadata": {},
   "source": [
    "sample  \n",
    "![sample](./test/rough/sample.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11e60ee-d3be-4e93-914f-ab2efec1d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "def export_to_onnx(model, input_size, onnx_path=\"model.onnx\"):\n",
    "    # 1. CPUã¸ç§»å‹•\n",
    "    model = model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # 2. å…¥åŠ›ã‚‚CPU\n",
    "    x = torch.randn(*input_size, device=\"cpu\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model, x, onnx_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output']\n",
    "    )\n",
    "    print(f\"[OK] Exported ONNX -> {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddddc29-ab5b-4925-af0e-0c3057123fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_20.pth\n",
      "[OK] Exported ONNX -> model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh1/deepl/mynovel/venv/lib/python3.10/site-packages/torch/onnx/symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"checkpoints/model_epoch_20.pth\")\n",
    "export_to_onnx(model, (1,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cea07c-1e27-40d7-bc21-6a6d7db15076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttention(512)\n",
    "dummy = torch.randn(1, 512, 32, 32)\n",
    "\n",
    "traced = torch.jit.trace(model, dummy)\n",
    "traced.save(\"selfattention.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f1561-299d-4029-bb1e-6c3c5d7180cc",
   "metadata": {},
   "source": [
    "![attn](./graph/selfattention.pt.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a658467-fbde-4f1e-9923-da9046738e2b",
   "metadata": {},
   "source": [
    "ğŸ” ã“ã® Self-Attention ã¯ä½•ï¼Ÿ\n",
    "\n",
    "ã“ã‚Œã¯ Non-Local Attentionï¼ˆSelf-Attentionï¼‰ ã®å…¸å‹å®Ÿè£…ã§ã€\n",
    "ç‰¹å¾´ãƒãƒƒãƒ—å†…ã® ã™ã¹ã¦ã®ä½ç½®åŒå£«ã®é–¢é€£æ€§ã‚’è¨ˆç®—ã—ã¦ã€\n",
    "ã€Œã©ã“ãŒã©ã“ã¨é–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã€ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "ç·šç”»ç”Ÿæˆã ã¨ï¼š\n",
    "\n",
    "é ãé›¢ã‚ŒãŸç·šã®æ–¹å‘æ€§\n",
    "\n",
    "åŒã˜æ§‹é€ ã®åå¾©\n",
    "\n",
    "é•·ã„è¼ªéƒ­ç·šã®ã¤ãªãŒã‚Š\n",
    "\n",
    "ãªã©ã‚’è£œå®Œã§ãã‚‹ã®ã§ U-Net ã«ã‚ˆãä½¿ã‚ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f1790-3e37-4600-9d62-1495327b75b5",
   "metadata": {},
   "source": [
    "Self-Attentionï¼ˆNon-Local Blockï¼‰ã‚’ U-Net ã«è¿½åŠ ã—ãŸå ´åˆã€å®Ÿè³ªçš„ã« â€œæ–°ã—ã„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã»ã¼ç”Ÿã¾ã‚Œã¾ã›ã‚“ã€‚â€\n",
    "ãŸã ã—ã€è¨­ç½®ä½ç½®ãƒ»æ•°ãƒ»in_dimï¼ˆãƒãƒ£ãƒãƒ«ï¼‰ãƒ»å­¦ç¿’ç‡ã¸ã®å½±éŸ¿ ã¨ã„ã† â€œæº–ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿â€ ã¯å­˜åœ¨ã—ã¾ã™ã€‚ \n",
    "\n",
    "â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆæº–ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
    "\n",
    "ã©ã®å±¤ã«å…¥ã‚Œã‚‹ã‹ï¼ˆæœ€é‡è¦ï¼‰\n",
    "\n",
    "ä½•å€‹å…¥ã‚Œã‚‹ã‹\n",
    "\n",
    "å­¦ç¿’ç‡ã‚’å¤‰ãˆã‚‹ã‹ï¼ˆå ´åˆã«ã‚ˆã‚Šï¼‰\n",
    "\n",
    "in_dimï¼ˆãƒãƒ£ãƒãƒ«æ•°ï¼‰ã®åˆã†å±¤ã‚’é¸ã¶å¿…è¦\n",
    "\n",
    "â˜… ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´ã—ãªãã¦ã‚ˆã„ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãªã„ï¼‰\n",
    "\n",
    "Î³\n",
    "\n",
    "attention ã®æ¸©åº¦\n",
    "\n",
    "head æ•°\n",
    "\n",
    "dropout\n",
    "\n",
    "kernel size\n",
    "\n",
    "scaling factor\n",
    "\n",
    "ã“ã‚Œã‚‰ã¯ã™ã¹ã¦è‡ªå‹•ã§æ©Ÿèƒ½ã™ã‚‹ã®ã§è§¦ã‚‰ãªã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d1f2dbb-bbfd-4920-9385-01efae2610a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attnres_graph.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttnResUNet()\n",
    "dummy = torch.randn(1,1,256,256)\n",
    "\n",
    "out = model(dummy)\n",
    "dot = make_dot(out, params=dict(model.named_parameters()))\n",
    "dot.render(\"attnres_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9651b30-942d-4c75-a84d-62557b36039d",
   "metadata": {},
   "source": [
    "![attnres](./graph/attnres_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d46e5d-059c-4ee0-837c-e6870a99018f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
