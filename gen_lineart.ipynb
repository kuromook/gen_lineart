{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5a8a05-c7fc-40b7-9c24-247ed843ddeb",
   "metadata": {},
   "source": [
    "+ [x] Edge Loss ã®èª¿æ•´ï¼šw_edge ã‚’èª¿æ•´ã—ã¦â€œç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•â€ã¨â€œãƒŽã‚¤ã‚ºâ€ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å–ã£ã¦ãã ã•ã„ï¼ˆ5ã€œ20ã®ç¯„å›²ã§è©¦ã™ã¨è‰¯ã„ï¼‰ã€‚\n",
    "\n",
    "+ [ ] Perceptual Lossï¼ˆVGGï¼‰ã‚’åŠ ãˆã‚‹ã¨ç·šã®å¤ªã•ã‚„â€œè¦‹ãŸç›®â€ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼ˆãŸã ã—å­¦ç¿’ãŒè¤‡é›‘ã«ï¼‰ã€‚torchvision.models.vgg19(pretrained=True) ã®ä¸­é–“ç‰¹å¾´ã§ L1 ã‚’å–ã‚‹æ–¹å¼ã€‚\n",
    "\n",
    "+ [ ] PatchGAN åˆ¤åˆ¥å™¨ï¼ˆGAN Lossï¼‰ ã‚’è¿½åŠ ã™ã‚‹ã¨æ›´ã«ã‚·ãƒ£ãƒ¼ãƒ—ã§è‡ªç„¶ãªç·šãŒå‡ºã›ã¾ã™ã€‚å®‰å®šåŒ–ã«ã¯ History Buffer/LSGAN/PatchGAN ãŒä½¿ã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Deep Supervisionï¼šè¤‡æ•°è§£åƒåº¦ã§å‡ºåŠ›ã—ã€ãã‚Œãžã‚Œã«æå¤±ã‚’ä»˜ã‘ã‚‹ã¨ç´°ç·šã®ç¶­æŒã«æœ‰åŠ¹ã§ã™ã€‚\n",
    "\n",
    "+ [ ] Data augmentï¼šç·šã®å¤ªã•ã‚„æ˜Žåº¦å¤‰åŒ–ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ãªã©ã‚’å…¥ã‚Œã‚‹ã¨æ±ŽåŒ–ãŒè‰¯ããªã‚Šã¾ã™ã€‚\n",
    "\n",
    "+ [ ] Inference TTAï¼šå·¦å³åè»¢ã‚„ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¹³å‡åŒ–ã—ã¦æœ€çµ‚å‡ºåŠ›ã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df15c0cf-415f-4d0d-9a1d-b798d4af465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import math\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5344ef-5d27-4f53-b12d-d0afc940d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utility blocks\n",
    "# -----------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=not use_bn)]\n",
    "        if use_bn:\n",
    "            layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(ch, ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, 3, 1, 1, bias=False)\n",
    "        self.norm = nn.InstanceNorm2d(ch, affine=True)\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm(out)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "# -----------------------\n",
    "# Simple Attention Gate (spatial attention)\n",
    "# -----------------------\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "    def forward(self, x):\n",
    "        # channel-wise average and max\n",
    "        avg = x.mean(dim=1, keepdim=True)\n",
    "        mx, _ = x.max(dim=1, keepdim=True)\n",
    "        cat = torch.cat([avg, mx], dim=1)  # 2 x H x W\n",
    "        # reduce to single-channel attention\n",
    "        att = torch.sigmoid(self.conv(cat))\n",
    "        return x * att\n",
    "\n",
    "# -----------------------\n",
    "# Basic Self-Attention module (non-local)\n",
    "# -----------------------\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.key_conv   = nn.Conv2d(in_dim, in_dim//8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        proj_query = self.query_conv(x).view(b, -1, h*w).permute(0,2,1)  # B x N x C'\n",
    "        proj_key   = self.key_conv(x).view(b, -1, h*w)                    # B x C' x N\n",
    "        energy = torch.bmm(proj_query, proj_key)                          # B x N x N\n",
    "        att = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value_conv(x).view(b, -1, h*w)                  # B x C x N\n",
    "        out = torch.bmm(proj_value, att.permute(0,2,1))                   # B x C x N\n",
    "        out = out.view(b, c, h, w)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# U-Net with Residual and Attention\n",
    "# -----------------------\n",
    "class AttnResUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, base_ch=64, use_self_attn=True):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(ConvBlock(in_ch, base_ch), ResidualBlock(base_ch))\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(ConvBlock(base_ch, base_ch*2), ResidualBlock(base_ch*2))\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(ConvBlock(base_ch*2, base_ch*4), ResidualBlock(base_ch*4))\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(ConvBlock(base_ch*4, base_ch*8), ResidualBlock(base_ch*8))\n",
    "        self.self_attn = SelfAttention(base_ch*8) if use_self_attn else nn.Identity()\n",
    "\n",
    "        # decoder\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 2, 2)\n",
    "        self.dec3 = nn.Sequential(ConvBlock(base_ch*8, base_ch*4), ResidualBlock(base_ch*4))\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, 2)\n",
    "        self.dec2 = nn.Sequential(ConvBlock(base_ch*4, base_ch*2), ResidualBlock(base_ch*2))\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, 2, 2)\n",
    "        self.dec1 = nn.Sequential(ConvBlock(base_ch*2, base_ch), ResidualBlock(base_ch))\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, out_ch, 1)\n",
    "        self.spatial_attn1 = SpatialAttention()\n",
    "        self.spatial_attn2 = SpatialAttention()\n",
    "        self.spatial_attn3 = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)           # B,64,H,W\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.enc2(p1)          # B,128,H/2,W/2\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.enc3(p2)          # B,256,H/4,W/4\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        b = self.bottleneck(p3)     # B,512,H/8,W/8\n",
    "        b = self.self_attn(b)\n",
    "\n",
    "        u3 = self.up3(b)            # B,256,H/4,W/4\n",
    "        # attention on skip\n",
    "        e3_att = self.spatial_attn1(e3)\n",
    "        d3 = self.dec3(torch.cat([u3, e3_att], dim=1))\n",
    "\n",
    "        u2 = self.up2(d3)\n",
    "        e2_att = self.spatial_attn2(e2)\n",
    "        d2 = self.dec2(torch.cat([u2, e2_att], dim=1))\n",
    "\n",
    "        u1 = self.up1(d2)\n",
    "        e1_att = self.spatial_attn3(e1)\n",
    "        d1 = self.dec1(torch.cat([u1, e1_att], dim=1))\n",
    "\n",
    "        out = self.final(d1)\n",
    "        out = torch.sigmoid(out)  # output in [0,1]\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Sobel (Edge) loss\n",
    "# -----------------------\n",
    "class SobelEdgeLoss(nn.Module):\n",
    "    def __init__(self, p=1):\n",
    "        super().__init__()\n",
    "        # Sobel kernels (horizontal, vertical)\n",
    "        kx = torch.tensor([[1, 0, -1],\n",
    "                           [2, 0, -2],\n",
    "                           [1, 0, -1]], dtype=torch.float32) / 4.0\n",
    "        ky = torch.tensor([[1, 2, 1],\n",
    "                           [0, 0, 0],\n",
    "                           [-1, -2, -1]], dtype=torch.float32) / 4.0\n",
    "        self.register_buffer('kx', kx.view(1,1,3,3))\n",
    "        self.register_buffer('ky', ky.view(1,1,3,3))\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred/target: Bx1xHxW\n",
    "        kx = self.kx.to(device=pred.device, dtype=pred.dtype)\n",
    "        ky = self.ky.to(device=pred.device, dtype=pred.dtype)\n",
    "\n",
    "        gx_pred = F.conv2d(pred, kx, padding=1)\n",
    "        gy_pred = F.conv2d(pred, ky, padding=1)\n",
    "        gx_tgt  = F.conv2d(target, kx, padding=1)\n",
    "        gy_tgt  = F.conv2d(target, ky, padding=1)\n",
    "\n",
    "        edge_pred = torch.sqrt(gx_pred**2 + gy_pred**2 + 1e-6)\n",
    "        edge_tgt  = torch.sqrt(gx_tgt**2 + gy_tgt**2 + 1e-6)\n",
    "\n",
    "        if self.p == 1:\n",
    "            return F.l1_loss(edge_pred, edge_tgt)\n",
    "        else:\n",
    "            return F.mse_loss(edge_pred, edge_tgt)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Small demo dataset (grayscale pairs)\n",
    "# Replace with your pair dataset: (rough_sketch, line_art)\n",
    "# -----------------------\n",
    "class PairImageDataset(Dataset):\n",
    "    def __init__(self, rough_dir, line_dir, size=(256,256)):\n",
    "        super().__init__()\n",
    "        self.rough_files = sorted(glob.glob(os.path.join(rough_dir, '*.jpg')))\n",
    "        self.line_files  = sorted(glob.glob(os.path.join(line_dir, '*.jpg')))\n",
    "        assert len(self.rough_files) == len(self.line_files), \"pair counts must match\"\n",
    "        self.tr = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    def __len__(self): return len(self.rough_files)\n",
    "    def __getitem__(self, idx):\n",
    "        r = Image.open(self.rough_files[idx]).convert('RGB')\n",
    "        l = Image.open(self.line_files[idx]).convert('RGB')\n",
    "        return self.tr(r), self.tr(l)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def train(\n",
    "    rough_dir='dataset/train/rough', line_dir='dataset/train/line',\n",
    "    epochs=50, batch_size=8, lr=1e-4, device='cuda'\n",
    "):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = PairImageDataset(rough_dir, line_dir)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=True).to(device)\n",
    "    edge_loss = SobelEdgeLoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(loader))\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()  # mixed precision\n",
    "    save_dir = 'checkpoints'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # loss weights\n",
    "    w_l1 = 1.0\n",
    "    w_edge = 10.0   # emphasize edges\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (rough, line) in enumerate(loader):\n",
    "            rough = rough.to(device)\n",
    "            line = line.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(rough)\n",
    "                loss_l1 = l1_loss(out, line)\n",
    "                loss_edge = edge_loss(out, line)\n",
    "                loss = w_l1 * loss_l1 + w_edge * loss_edge\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % 50 == 0:\n",
    "                print(f\"Epoch[{epoch}/{epochs}] Iter[{i+1}/{len(loader)}] Loss: {running_loss / (i+1):.4f}\")\n",
    "\n",
    "        avg = running_loss / len(loader)\n",
    "        print(f\"==> Epoch {epoch} finished. avg loss: {avg:.4f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict()\n",
    "        }, os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a6d5e-7092-45d6-8705-ead8e0319174",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: point to your directories containing paired PNGs\n",
    "    train(\n",
    "        rough_dir='dataset/train/rough',   # ä¸‹çµµç”»åƒãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpngï¼‰\n",
    "        line_dir='dataset/train/line',     # æ­£è§£ç·šç”»ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpngï¼‰\n",
    "        epochs=30,\n",
    "        batch_size=8,\n",
    "        lr=2e-4,\n",
    "        device='cuda'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d36615-7b3a-4232-af7e-410c5dd516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "def load_model(checkpoint_path, device='cuda'):\n",
    "    model = AttnResUNet(in_ch=1, out_ch=1, base_ch=64, use_self_attn=True)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"âœ… Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img_path, size=(256,256)):\n",
    "    tr = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    return tr(img).unsqueeze(0)  # 1x1xHxW tensor\n",
    "\n",
    "def postprocess_and_save(tensor, save_path):\n",
    "    # tensor: 1x1xHxW in [0,1]\n",
    "    img = tensor.squeeze(0).squeeze(0).detach().cpu()  # HxW\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.save(save_path)\n",
    "    print(f\"ðŸ’¾ Saved output to {save_path}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(model, input_path, output_path, device='cuda'):\n",
    "    img = preprocess_image(input_path)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        output = model(img)\n",
    "\n",
    "    postprocess_and_save(output, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d6a51-9264-41a3-a731-4f66037a3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # ======== è¨­å®š =========\n",
    "    checkpoint_path = \"checkpoints/model_epoch_30.pth\"  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n",
    "    input_dir = \"test/rough\"   # ãƒ†ã‚¹ãƒˆç”¨ã®ä¸‹çµµ\n",
    "    output_dir = \"results\"             # å‡ºåŠ›å…ˆ\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = load_model(checkpoint_path, device)\n",
    "\n",
    "    # ======== æŽ¨è«– =========\n",
    "    rough_files = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    for fname in rough_files:\n",
    "        in_path = os.path.join(input_dir, fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        inference(model, in_path, out_path, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf0af1-ea5e-456d-8c08-7dd674e87b92",
   "metadata": {},
   "source": [
    "| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿    | åŠ¹æžœ                     | å‚™è€ƒ                     |\n",
    "| -------- | ---------------------- | ---------------------- |\n",
    "| `w_l1`   | å…¨ä½“ã®æ˜Žã‚‹ã•ã‚„å½¢ã®å†ç¾é‡è¦–          | é€šå¸¸ 1.0 ã§OK             |\n",
    "| `w_edge` | ã‚¨ãƒƒã‚¸ï¼ˆç·šã®ã‚·ãƒ£ãƒ¼ãƒ—ã•ï¼‰ã‚’ã©ã‚Œã ã‘é‡è¦–ã™ã‚‹ã‹ | å¤§ãã„ã»ã©ç·šãŒãã£ãã‚Šã€‚ãŸã ã—ãƒŽã‚¤ã‚ºã‚‚å¢—ãˆã‚‹ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a2260-cc1e-463b-b91e-c9205c962946",
   "metadata": {},
   "source": [
    "| ä¾‹      | è¨­å®š               | åŠ¹æžœã®å‚¾å‘                 |\n",
    "| ------ | ---------------- | --------------------- |\n",
    "| æŸ”ã‚‰ã‹ã‚   | `w_edge = 3.0`   | ç·šãŒã‚„ã‚„ã¼ã‚„ã‘ã‚‹ãŒãƒŽã‚¤ã‚ºå°‘ãªã„       |\n",
    "| æ¨™æº–     | `w_edge = 5.0`   | ãƒãƒ©ãƒ³ã‚¹åž‹ï¼ˆä»Šã®è¨­å®šï¼‰           |\n",
    "| ã‚·ãƒ£ãƒ¼ãƒ—å¼·èª¿ | `w_edge = 10.0`  | ç·šãŒãã£ãã‚Šã€ã‚„ã‚„ãƒŽã‚¤ã‚ºå‡ºã‚‹        |\n",
    "| å¼·ã‚     | `w_edge = 15ã€œ20` | ã‚¨ãƒƒã‚¸æœ€é‡è¦–ã€ç·šã¯æ¿ƒã„ãŒãƒŽã‚¤ã‚ºãŒå¢—ãˆã‚‹å‚¾å‘ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890dab9-fb7f-497d-a3a0-4ff0551fac16",
   "metadata": {},
   "source": [
    "result\n",
    "\n",
    "![result](./results/sample_w_edge10.jpg) 10\n",
    "![result_w_edge20](./results/sample_w_edge20.jpg) 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2721061-310d-4cb1-ab55-c83b623e9196",
   "metadata": {},
   "source": [
    "sample  \n",
    "![sample](./test/rough/sample.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11e60ee-d3be-4e93-914f-ab2efec1d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "def export_to_onnx(model, input_size, onnx_path=\"model.onnx\"):\n",
    "    # 1. CPUã¸ç§»å‹•\n",
    "    model = model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # 2. å…¥åŠ›ã‚‚CPU\n",
    "    x = torch.randn(*input_size, device=\"cpu\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model, x, onnx_path,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output']\n",
    "    )\n",
    "    print(f\"[OK] Exported ONNX -> {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddddc29-ab5b-4925-af0e-0c3057123fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_20.pth\n",
      "[OK] Exported ONNX -> model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh1/deepl/mynovel/venv/lib/python3.10/site-packages/torch/onnx/symbolic_helper.py:1513: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"checkpoints/model_epoch_20.pth\")\n",
    "export_to_onnx(model, (1,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3b2b1a-c379-4a56-9011-46ced6ae803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "def visualize_with_torchviz(model, input_size=(1,1,256,256), filename=\"unet_graph\"):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.randn(*input_size).to(device)\n",
    "\n",
    "    y = model(x)\n",
    "\n",
    "    dot = make_dot(y, params=dict(model.named_parameters()))\n",
    "    dot.format = \"png\"\n",
    "    dot.render(filename, cleanup=True)\n",
    "    print(f\"âœ” Saved model graph: {filename}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90b82d0-215d-426f-ac08-344dee64b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint from checkpoints/model_epoch_20.pth\n",
      "âœ” Saved model graph: unet_graph.png\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"checkpoints/model_epoch_20.pth\")\n",
    "visualize_with_torchviz(model, (1,1,256,256), \"unet_graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce375c55-ff28-4e2c-89ba-fa94b42d846b",
   "metadata": {},
   "source": [
    "![graph](./unet_graph.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
